# Interval-Linear-Regression
A regression algorithm based on splitting a dataset into intervals and calculating linear regression coefficients for each interval.
The number of intervals into which the sorted dataset is divided, as well as the number of iterations, is specified by the user (k and iterations). Then the algorithm, by finding the optimal partition of the data into k intervals, calculates the linear regression coefficients for each interval. Each interval is approximated by a straight line and the linear regression coefficients are found using batch gradient descent. The criterion for finding the best optimal splitting is the error (which is equal to the sum of the squares of the difference between the real value of y and regression line, divided by the size of the data set). The partition with the smallest error from all iterations will be used. This algorithm is an average between the usual linear regression and polynomial regression. Compared to linear regression, it should ideally be less subject to underfitting, and compared to polynomial regression, it should be less prone to overfitting. Here is the implementation of the interval linear regression algorithm for a one-dimensional dataset y = f(x). Work with an algorithm that manages with many features is currently underway.
